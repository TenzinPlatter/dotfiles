#!/usr/bin/env python3
"""CLI tool for interacting with local ollama models."""

import sys
import json
import urllib.request
import urllib.parse
import urllib.error
import subprocess
import argparse


def stream_ollama_response(model, prompt):
    """Send a prompt to ollama and stream the response."""
    url = "http://localhost:11434/api/generate"
    data = json.dumps({"model": model, "prompt": prompt, "stream": True}).encode("utf-8")

    request = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})

    try:
        with urllib.request.urlopen(request) as response:
            buffer = ""
            while True:
                chunk = response.read(1024)
                if not chunk:
                    break

                # Decode and add to buffer
                buffer += chunk.decode("utf-8")

                # Process complete lines
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    if line.strip():
                        try:
                            json_chunk = json.loads(line)
                            if "response" in json_chunk:
                                print(json_chunk["response"], end="", flush=True)
                        except json.JSONDecodeError:
                            # Skip malformed JSON
                            continue
    except urllib.error.URLError as e:
        print("Error: Could not connect to ollama. Is it running? (ollama serve)", file=sys.stderr)
        raise


def stream_chat_response(model, messages):
    """Send messages to ollama chat endpoint and stream the response.

    Returns the assistant's response text for context tracking.
    """
    url = "http://localhost:11434/api/chat"
    data = json.dumps({"model": model, "messages": messages, "stream": True}).encode("utf-8")

    request = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})

    response_text = ""
    try:
        with urllib.request.urlopen(request) as response:
            buffer = ""
            while True:
                chunk = response.read(1024)
                if not chunk:
                    break

                # Decode and add to buffer
                buffer += chunk.decode("utf-8")

                # Process complete lines
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    if line.strip():
                        try:
                            json_chunk = json.loads(line)
                            if "message" in json_chunk and "content" in json_chunk["message"]:
                                content = json_chunk["message"]["content"]
                                print(content, end="", flush=True)
                                response_text += content
                        except json.JSONDecodeError:
                            # Skip malformed JSON
                            continue
    except urllib.error.URLError as e:
        print("Error: Could not connect to ollama. Is it running? (ollama serve)", file=sys.stderr)
        raise

    return response_text


def interactive_session(model):
    """Run an interactive chat session with context preservation."""
    messages = []

    print(f"Starting interactive session with {model}")
    print("Type your messages and press Enter. Use Ctrl+D or Ctrl+C to exit.")
    print("-" * 60)

    try:
        while True:
            try:
                # Read user input
                print("\nYou: ", end="", flush=True)
                user_input = input()

                if not user_input.strip():
                    continue

                # Add user message to context
                messages.append({"role": "user", "content": user_input})

                # Get and stream response
                print(f"\n{model}: ", end="", flush=True)
                response = stream_chat_response(model, messages)
                print()  # Newline after response

                # Add assistant response to context
                messages.append({"role": "assistant", "content": response})

            except EOFError:
                # Ctrl+D pressed
                print("\n\nExiting interactive session.")
                break
    except KeyboardInterrupt:
        # Ctrl+C pressed
        print("\n\nExiting interactive session.")

    return 0


def parse_build_errors(build_cmd, build_output):
    """Parse build errors using the 1B model and output in vim-compatible format."""
    prompt = f"""Parse this build output and extract only compilation errors.
Format each error exactly as: <filename>:<line>:<column>: <error>: <message>
Only output the formatted errors, nothing else.

Build command:
{build_cmd}

Build output:
{build_output}"""

    stream_ollama_response("llama3.2:1b", prompt)


def main(argv=None):
    """Main entry point for the llm script."""
    parser = argparse.ArgumentParser(
        description="CLI tool for interacting with local ollama models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  llm "what is 2+2"                 # Ask a question
  llm -i                            # Start interactive chat session
  llm --build make test             # Parse build errors
  llm --build -i make test          # Build in interactive shell
        """,
    )

    parser.add_argument(
        "--build", "-b", action="store_true", help="Build mode: run command and parse errors"
    )

    parser.add_argument(
        "--interactive",
        "-i",
        action="store_true",
        help="Interactive mode: Start a chat session with context OR run build command in interactive shell (zsh)",
    )

    parser.add_argument("args", nargs="*", help="Prompt for basic mode or command for build mode")

    args = parser.parse_args(argv)

    # Interactive chat session mode
    if args.interactive and not args.build:
        if args.args:
            print("Error: Interactive chat mode does not take arguments", file=sys.stderr)
            return 1
        return interactive_session("llama3.2:3b")

    # Validate arguments for other modes
    if not args.args:
        parser.print_help(sys.stderr)
        return 1

    # Build mode
    if args.build:
        build_cmd = args.args

        # Run build command
        if args.interactive:
            # Run through shell for interactive features
            cmd_str = " ".join(build_cmd)
            result = subprocess.run(
                cmd_str, shell=True, capture_output=True, text=True, executable="/bin/zsh"
            )
        else:
            # Run directly
            result = subprocess.run(build_cmd, capture_output=True, text=True)

        # Combine stdout and stderr
        build_output = result.stdout + result.stderr

        # Parse errors with AI
        try:
            parse_build_errors(build_cmd, build_output)
        except urllib.error.URLError:
            return 1

        return result.returncode

    # Basic mode: answer questions
    else:
        prompt = " ".join(args.args)

        try:
            stream_ollama_response("llama3.2:3b", prompt)
        except urllib.error.URLError:
            return 1

        return 0


if __name__ == "__main__":
    sys.exit(main())
