#!/usr/bin/env python3
"""CLI tool for interacting with local ollama models."""

import sys
import json
import urllib.request
import urllib.parse
import urllib.error
import subprocess
import argparse


def stream_ollama_response(model, prompt):
    """Send a prompt to ollama and stream the response."""
    url = "http://localhost:11434/api/generate"
    data = json.dumps({"model": model, "prompt": prompt, "stream": True}).encode("utf-8")

    request = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})

    try:
        with urllib.request.urlopen(request) as response:
            buffer = ""
            while True:
                chunk = response.read(1024)
                if not chunk:
                    break

                # Decode and add to buffer
                buffer += chunk.decode("utf-8")

                # Process complete lines
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    if line.strip():
                        try:
                            json_chunk = json.loads(line)
                            if "response" in json_chunk:
                                print(json_chunk["response"], end="", flush=True)
                        except json.JSONDecodeError:
                            # Skip malformed JSON
                            continue
    except urllib.error.URLError as e:
        print("Error: Could not connect to ollama. Is it running? (ollama serve)", file=sys.stderr)
        raise


def parse_build_errors(build_output):
    """Parse build errors using the 1B model and output in vim-compatible format."""
    prompt = f"""Parse this build output and extract only compilation errors.
Format each error exactly as: filename:line:column: error: message
Only output the formatted errors, nothing else.

Build output:
{build_output}"""

    stream_ollama_response("llama3.2:1b", prompt)


def main(argv=None):
    """Main entry point for the llm script."""
    parser = argparse.ArgumentParser(
        description="CLI tool for interacting with local ollama models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  llm "what is 2+2"                 # Ask a question
  llm --build make test             # Parse build errors
  llm --build -i make test          # Build in interactive shell
        """,
    )

    parser.add_argument(
        "--build", "-b", action="store_true", help="Build mode: run command and parse errors"
    )

    parser.add_argument(
        "--interactive",
        "-i",
        action="store_true",
        help="Run build command in interactive shell (zsh)",
    )

    parser.add_argument("args", nargs="*", help="Prompt for basic mode or command for build mode")

    args = parser.parse_args(argv)

    # Validate arguments
    if not args.args:
        parser.print_help(sys.stderr)
        return 1

    if args.interactive and not args.build:
        print("Error: -i/--interactive can only be used with --build", file=sys.stderr)
        return 1

    # Build mode
    if args.build:
        build_cmd = args.args

        # Run build command
        if args.interactive:
            # Run through shell for interactive features
            cmd_str = " ".join(build_cmd)
            result = subprocess.run(
                cmd_str, shell=True, capture_output=True, text=True, executable="/bin/zsh"
            )
        else:
            # Run directly
            result = subprocess.run(build_cmd, capture_output=True, text=True)

        # Combine stdout and stderr
        build_output = result.stdout + result.stderr

        # Parse errors with AI
        try:
            parse_build_errors(build_output)
        except urllib.error.URLError:
            return 1

        return result.returncode

    # Basic mode: answer questions
    else:
        prompt = " ".join(args.args)

        try:
            stream_ollama_response("llama3.2:3b", prompt)
        except urllib.error.URLError:
            return 1

        return 0


if __name__ == "__main__":
    sys.exit(main())
